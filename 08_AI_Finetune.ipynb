{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hby1117/class2025Spring/blob/main/08_AI_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccbA0CejRsV-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "from datasets import Dataset\n",
        "from transformers import TextStreamer\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "N8TzThHXu07L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po4mMJ6chEmJ"
      },
      "source": [
        "# Data\n",
        "\n",
        "\n",
        "* raw\n",
        "> In the latest AP dispatch from Frankfurt, which details how the European Union's chief trade negotiator said he had “good calls” with Trump administration officials on Monday, David McHugh noted that the EU \"has offered Trump a 'zero for zero' deal in which tariffs would be removed on industrial goods including automobiles, but the U.S. administration has said it will not lower tariffs below a 10% baseline imposed on almost all its trading partners. Trump has also announced tariffs of 25% on steel and automobiles.\"\n",
        "That raises the question of where the negotiations, which got back on the rails over the weekend after a call between President Trump and EU Commission President Ursula von der Leyen, will go from here — and what Trump will have to say next time he discusses the $1.8 trillion trade relationship.\n",
        "\n",
        "* pretrain\n",
        "\t-\tone record:\n",
        "\t\t> In the latest AP dispatch from Frankfurt, which details how the European Union's chief trade negotiator said he had “good calls” with Trump administration officials on Monday, David McHugh noted that the EU \"has offered Trump a 'zero for zero' deal in which tariffs would be removed on industrial goods including automobiles, but the U.S. administration has said it will not lower tariffs below a 10% baseline imposed on almost all its trading partners. Trump has also announced tariffs of 25% on steel and automobiles.\"\n",
        "\t\tThat raises the question of where the negotiations, which got back on the rails over the weekend after a call between President Trump and EU Commission President Ursula von der Leyen, will go from here — and what Trump will have to say next time he discusses the $1.8 trillion trade relationship.\n",
        "\n",
        "* instructional - alpaca\n",
        "```\n",
        "[\n",
        "\t{\n",
        "\t\t\"Instruction\": \"Task we want the model to perform. 11111\",\n",
        "\t\t\"Input\": \"Optional, but useful, it will essentially be the user's query. 11111\",\n",
        "\t\t\"Output\": \"The expected result of the task and the output of the model 11111.\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"Instruction\": \"Task we want the model to perform. 22222\",\n",
        "\t\t\"Input\": \"Optional, but useful, it will essentially be the user's query. 22222\",\n",
        "\t\t\"Output\": \"The expected result of the task and the output of the model. 22222\"\n",
        "\t},\n",
        "]\n",
        "```\n",
        "\n",
        "* instructional - full fine-tuning | Lora fine-tuning\n",
        "\t- one record:\n",
        "\t\t```\n",
        "\t\t\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\t\tWrite a response that appropriately completes the request.\n",
        "\n",
        "\t\t### Instruction:\n",
        "\t\tTask we want the model to perform. 11111\n",
        "\n",
        "\t\t### Input:\n",
        "\t\tOptional, but useful, it will essentially be the user's query. 11111\n",
        "\n",
        "\t\t### Response:\n",
        "\t\tThe expected result of the task and the output of the model. 11111\"\"\"\n",
        "\t\t```\n",
        "\t- one record:\n",
        "\t\t```\n",
        "\t\t\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\t\tWrite a response that appropriately completes the request.\n",
        "\n",
        "\t\t### Instruction:\n",
        "\t\tTask we want the model to perform. 22222\n",
        "\n",
        "\t\t### Input:\n",
        "\t\tOptional, but useful, it will essentially be the user's query. 22222\n",
        "\n",
        "\t\t### Response:\n",
        "\t\tThe expected result of the task and the output of the model. 22222\"\"\"\n",
        "\t\t```\n",
        "\n",
        "* conversational - shareGPT\n",
        "```\n",
        "{\n",
        "\t\"conversations\": [\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"system\",\n",
        "\t\t\t\"value\": \"You are a helpful assistant.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"human\",\n",
        "\t\t\t\"value\": \"Can you help me make pasta carbonara?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"gpt\",\n",
        "\t\t\t\"value\": \"Would you like the traditional Roman recipe, or a simpler version?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"human\",\n",
        "\t\t\t\"value\": \"The traditional version please\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"from\": \"gpt\",\n",
        "\t\t\t\"value\": \"The authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "```\n",
        "\n",
        "* conversational (shareGPT): full fine-tuning | Lora fine-tuning\n",
        "\t- one record:\n",
        "\t\t```\n",
        "\t\t<|im_start|>system\n",
        "\t\tYou are a helpful assistant.<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tCan you help me make pasta carbonara?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tWould you like the traditional Roman recipe, or a simpler version?<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tThe traditional version please<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tThe authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?<|im_end|>\n",
        "\t\t```\n",
        "\n",
        "* conversational - ChatML\n",
        "```\n",
        "{\n",
        "\t\"messages\": [\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"system\",\n",
        "\t\t\t\"content\": \"You are a helpful assistant.\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"What is 1+1?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"assistant\",\n",
        "\t\t\t\"content\": \"It's 2!\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"What is 2+2?\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"assistant\",\n",
        "\t\t\t\"content\": \"It's 4!\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "```\n",
        "\n",
        "* conversational (ChatML): full fine-tuning | Lora fine-tuning\n",
        "\t- one record:\n",
        "\t\t```\n",
        "\t\t<|im_start|>system\n",
        "\t\tYou are a helpful assistant.<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tWhat is 1+1?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tIt's 2!<|im_end|>\n",
        "\t\t<|im_start|>user\n",
        "\t\tWhat is 2+2?<|im_end|>\n",
        "\t\t<|im_start|>assistant\n",
        "\t\t<think>\n",
        "\t\t</think>\n",
        "\t\tIt's 4!<|im_end|>\n",
        "\t\t```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCFFT_O4g-dB"
      },
      "outputs": [],
      "source": [
        "with open('class-2025-spring_0530-0.json', 'r') as f:\n",
        "    loaded_data = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_list(loaded_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "## Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "# Data Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp"
      },
      "outputs": [],
      "source": [
        "organized_dataset = standardize_sharegpt(dataset)\n",
        "processed_texts = tokenizer.apply_chat_template(\n",
        "    organized_dataset[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")\n",
        "df = pd.DataFrame({\"text\": processed_texts})\n",
        "processed_dataset = Dataset.from_pandas(df)\n",
        "processed_dataset = processed_dataset.shuffle(seed = 3407)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = processed_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "# Inference\n",
        "\n",
        "According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"자유심증주의란 무엇인가요? 어떻게 적용되나요? 헷갈리는데?\"},\n",
        "    # {\"role\" : \"user\", \"content\" : \"경복궁이 뭐에요?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "output = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "# Saving & loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"my_lora_model\")\n",
        "tokenizer.save_pretrained(\"my_lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Loading trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"my_lora_model\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}